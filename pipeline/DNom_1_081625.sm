# File: Snakefile

import os
import pandas as pd
from pathlib import Path

# Load configuration
configfile: "config.yaml"

# Validate paired parameter
if "paired" not in config:
    raise ValueError("'paired' parameter must be specified in config.yaml")
if not isinstance(config["paired"], bool):
    raise ValueError("'paired' must be a boolean value (TRUE or FALSE)")

# Validate embedding parameter - UPDATED to support "ALL" option
VALID_EMBEDDING_OPTIONS = ["RTD", "CGR", "ALL"]
if "embedding" not in config:
    raise ValueError("'embedding' parameter must be specified in config.yaml")
if config["embedding"] not in VALID_EMBEDDING_OPTIONS:
    raise ValueError(f"'embedding' must be one of: {VALID_EMBEDDING_OPTIONS}")

# Determine which embeddings to use based on config - NEW LOGIC
if config["embedding"] == "ALL":
    EMBEDDINGS_TO_USE = ["RTD", "CGR"]
    print(f"Pipeline will run BOTH embedding methods: RTD and CGR")
else:
    EMBEDDINGS_TO_USE = [config["embedding"]]
    print(f"Pipeline will run single embedding method: {config['embedding']}")

# Validate threads parameter
if "threads" not in config:
    raise ValueError("'threads' parameter must be specified in config.yaml")
if not isinstance(config["threads"], int) or config["threads"] <= 0:
    raise ValueError("'threads' must be a positive integer")

# Validate strand parameter
if "strand" not in config:
    raise ValueError("'strand' parameter must be specified in config.yaml")

# Validate RTD-specific parameters if RTD is being used - UPDATED
if "RTD" in EMBEDDINGS_TO_USE:
    if "k" not in config:
        raise ValueError("'k' parameter must be specified when using RTD embedding")
    if "alphabet" not in config:
        raise ValueError("'alphabet' parameter must be specified when using RTD embedding")

# Validate clustering parameters
clustering_params = ["min_cluster_size", "min_samples", "max_GMM", "seed"]
for param in clustering_params:
    if param not in config:
        raise ValueError(f"'{param}' parameter must be specified for clustering")

# Validate bootstrapping parameters
if "n_bootstraps" not in config:
    raise ValueError("'n_bootstraps' parameter must be specified for bootstrapping")
if not isinstance(config["n_bootstraps"], int) or config["n_bootstraps"] <= 0:
    raise ValueError("'n_bootstraps' must be a positive integer")

# Validate Sleuth parameters
sleuth_params = ["parameter", "pseudocount", "alpha", "foldchange"]
for param in sleuth_params:
    if param not in config:
        raise ValueError(f"'{param}' parameter must be specified for Sleuth analysis")

# Validate numeric Sleuth parameters
if not isinstance(config["pseudocount"], (int, float)) or config["pseudocount"] <= 0:
    raise ValueError("'pseudocount' must be a positive number")
if not isinstance(config["alpha"], float) or not 0 < config["alpha"] < 1:
    raise ValueError("'alpha' must be a float between 0 and 1")
if not isinstance(config["foldchange"], (int, float)) or config["foldchange"] <= 0:
    raise ValueError("'foldchange' must be a positive number")

# Define base directories
BASE_DIR = config["base_dir"]
INPUT_DIR = "input"

# Define a function to get directory paths based on embedding type
def get_dirs(embedding):
    """
    Create directory structure for each embedding method.
    Each method gets its own isolated directory tree to avoid conflicts.
    """
    dirs = {}
    
    # All embedding types have these directories
    if embedding in ["RTD", "CGR"]:
        dirs.update({
            "root": embedding,
            "embed_dir": f"{embedding}/embedding",
            "cluster_dir": f"{embedding}/clustering",
            "bootstrap_dir": f"{embedding}/bootstrapping",
            "sleuth_dir": f"{embedding}/Sleuth",
            "retrieval_dir": f"{embedding}/significant"
        })
    
    return dirs

# Create directory structure by embedding type
DIRS = {embedding: get_dirs(embedding) for embedding in EMBEDDINGS_TO_USE}

def get_expected_outputs():
    """
    Generate list of expected output files based on which embeddings are active.
    This function dynamically adjusts based on whether we're running one or both methods.
    """
    outputs = []
    
    # Common outputs that are always generated
    outputs.extend([
        "validation_complete.txt",
        "embedding_mode_reported.txt",
        f"{INPUT_DIR}/qc_merge_complete.txt"
    ])
    
    # Add outputs for each active embedding method
    for method in EMBEDDINGS_TO_USE:
        outputs.extend([
            f"{DIRS[method]['embed_dir']}/embedding_complete.txt",
            f"{DIRS[method]['cluster_dir']}/clustering_complete.txt",
            f"{DIRS[method]['bootstrap_dir']}/bootstrapping_complete.txt",
            f"{DIRS[method]['sleuth_dir']}/sleuth_complete.txt",
            f"{DIRS[method]['retrieval_dir']}/retrieval_complete.txt"
        ])
    
    return outputs

# Load metadata
def load_metadata():
    try:
        return pd.read_csv(config["metadata"], sep="\t")
    except Exception as e:
        raise ValueError(f"Error reading metadata file: {e}")

# Validate parameter exists in metadata
metadata = load_metadata()
if config["parameter"] not in metadata.columns:
    raise ValueError(f"Parameter '{config['parameter']}' not found in metadata file")

# Input validation function
def validate_input_structure():
    """
    Comprehensive validation of input data structure.
    Checks for proper directory organization and file presence.
    """
    # Check if base directory exists
    base_dir = Path(config["base_dir"])
    if not base_dir.exists():
        raise ValueError(f"Base directory {base_dir} does not exist")
    
    # Check if input directory exists
    input_dir = base_dir / INPUT_DIR
    if not input_dir.exists():
        raise ValueError(f"Input directory {input_dir} does not exist")
    
    # Check if metadata file exists
    metadata_path = base_dir / config["metadata"]
    if not metadata_path.exists():
        raise ValueError(f"Metadata file {metadata_path} does not exist")
    
    # Load and validate metadata
    metadata = load_metadata()
    required_columns = ["SRA", "description", "sample"]
    missing_columns = [col for col in required_columns if col not in metadata.columns]
    if missing_columns:
        raise ValueError(f"Missing required columns in metadata: {missing_columns}")
    
    # Check if QC/merge has been completed
    qc_merge_complete = (input_dir / "qc_merge_complete.txt").exists()
    
    # Validate input directory structure
    if qc_merge_complete:
        # After QC/merge, check for sample-named directories
        for sample in metadata["sample"].unique():
            sample_dir = input_dir / str(sample)
            if not sample_dir.exists():
                raise ValueError(f"Missing directory for sample: {sample_dir}")
    else:
        # Before QC/merge, check for SRA-named directories and fastq files
        for sra in metadata["SRA"]:
            sra_dir = input_dir / str(sra)
            if not sra_dir.exists():
                raise ValueError(f"Missing directory for SRA: {sra_dir}")
            
            # Check for fastq files based on paired status
            if config["paired"]:
                r1 = sra_dir / f"{sra}_1.fastq.gz"
                r2 = sra_dir / f"{sra}_2.fastq.gz"
                if not r1.exists():
                    raise ValueError(f"Missing R1 fastq file: {r1}")
                if not r2.exists():
                    raise ValueError(f"Missing R2 fastq file: {r2}")
            else:
                fastq = sra_dir / f"{sra}.fastq.gz"
                if not fastq.exists():
                    raise ValueError(f"Missing fastq file: {fastq}")

# Perform initial validation
validate_input_structure()

# Get list of all samples
metadata = load_metadata()
SAMPLES = metadata["SRA"].tolist()

# Define the target rule
rule all:
    input:
        get_expected_outputs()

# Validation rule
rule validate_inputs:
    output:
        touch("validation_complete.txt")
    run:
        validate_input_structure()
        print("Input validation completed successfully!")

# Rule to report embedding mode - UPDATED to reflect new modes
rule report_embedding_mode:
    output:
        touch("embedding_mode_reported.txt")
    run:
        # Report which mode we're running in
        if config["embedding"] == "ALL":
            print("\n" + "="*60)
            print("Pipeline will run in DUAL EMBEDDING mode:")
            print("Reads will be embedded using BOTH 'RTD' and 'CGR' methods")
            print("This will produce parallel analyses for comparison")
            print("="*60 + "\n")
        else:
            print("\n" + "="*60)
            print(f"Pipeline will run in SINGLE EMBEDDING mode:")
            print(f"Reads will be embedded using '{config['embedding']}' only")
            print("="*60 + "\n")
        
        # If RTD is being used, report its parameters
        if "RTD" in EMBEDDINGS_TO_USE:
            print("RTD embedding parameters:")
            print(f"  - k-mer size: {config['k']}")
            print(f"  - Alphabet: {config['alphabet']}\n")

rule qc_and_merge:
    input:
        "validation_complete.txt",
        "embedding_mode_reported.txt"
    output:
        touch(f"{INPUT_DIR}/qc_merge_complete.txt")
    params:
        pipeline_dir = config["pipeline_dir"],
        input_dir = os.path.join(config["base_dir"], INPUT_DIR),
        metadata = config["metadata"],
        threads = config["threads"],
        paired = "TRUE" if config["paired"] else "FALSE"
    shell:
        """
        echo "\nStarting read QC and merging..."
        echo "Using {params.threads} CPUs for processing"
        echo "Input directory: {params.input_dir}"
        echo "Pipeline directory: {params.pipeline_dir}"
        echo "Paired-end mode: {params.paired}\n"
        
        {params.pipeline_dir}/filter_merge.sh \
            -p {params.input_dir} \
            -m {params.metadata} \
            -t {params.threads} \
            --paired {params.paired}
        """

# Embedding rule - Works for both RTD and CGR based on wildcard
rule embedding:
    input:
        f"{INPUT_DIR}/qc_merge_complete.txt"
    output:
        "{embedding}/embedding/embedding_complete.txt"
    wildcard_constraints:
        embedding = "RTD|CGR"  # Ensures only RTD and CGR are valid values
    params:
        pipeline_dir = config["pipeline_dir"],
        base_dir = config["base_dir"],
        input_dir = INPUT_DIR,
        metadata = config["metadata"],
        threads = config["threads"],
        # Embedding-specific params handled with conditional logic
        k = lambda wildcards: config.get("k", None) if wildcards.embedding == "RTD" else None,
        alphabet = lambda wildcards: config.get("alphabet", None) if wildcards.embedding == "RTD" else None,
        strand = config["strand"],
        paired = "TRUE" if config["paired"] else "FALSE"
    shell:
        """
        echo ""
        echo "============================================================"
        echo "Starting {wildcards.embedding} embedding..."
        echo "============================================================"
        echo "Using {params.threads} CPUs for processing"
        echo "Input directory: {params.input_dir}"
        
        # Create the output directory
        mkdir -p $(dirname {output})
        
        # RTD-specific parameters and command
        if [ "{wildcards.embedding}" = "RTD" ]; then
            echo "Embedding method: Return Time Distribution (RTD)"
            echo "k-mer size: {params.k}"
            echo "Alphabet: {params.alphabet}"
            echo "Strand: {params.strand}"
            
            {params.pipeline_dir}/generate_rtd.sh \\
                -b {params.base_dir} \\
                -i {params.input_dir} \\
                -p {params.pipeline_dir} \\
                -m {params.metadata} \\
                -k {params.k} \\
                -a "{params.alphabet}" \\
                -s {params.strand} \\
                -t {params.threads} \\
                -o "$(dirname {output})" \\
                --paired {params.paired}
        else
            # CGR-specific command
            echo "Embedding method: Chaos Game Representation (CGR)"
            echo "Strand: {params.strand}"
            
            {params.pipeline_dir}/generate_cgr.sh \\
                -b {params.base_dir} \\
                -i {params.input_dir} \\
                -p {params.pipeline_dir} \\
                -m {params.metadata} \\
                -s {params.strand} \\
                -t {params.threads} \\
                -o "$(dirname {output})" \\
                --paired {params.paired}
        fi
        
        # Ensure the output file exists
        touch {output}
        echo "Completed {wildcards.embedding} embedding"
        """

# Clustering rule - Works for both methods independently
rule clustering:
    input:
        "{embedding}/embedding/embedding_complete.txt"
    output:
        "{embedding}/clustering/clustering_complete.txt"
    wildcard_constraints:
        embedding = "RTD|CGR"  # Only RTD and CGR
    params:
        pipeline_dir = config["pipeline_dir"],
        min_cluster_size = config["min_cluster_size"],
        min_samples = config["min_samples"],
        max_GMM = config["max_GMM"],
        threads = config["threads"],
        seed = config["seed"]
    shell:
        """
        echo ""
        echo "============================================================"
        echo "Starting {wildcards.embedding} clustering..."
        echo "============================================================"
        echo "Input directory: $(dirname {input})"
        echo "Output directory: $(dirname {output})"
        echo "Clustering parameters:"
        echo "  - Min cluster size: {params.min_cluster_size}"
        echo "  - Min samples: {params.min_samples}"
        echo "  - Max GMM components: {params.max_GMM}"
        echo "  - Random seed: {params.seed}"
        
        # Create output directory
        mkdir -p $(dirname {output})
        cd $(dirname {output})
        
        # Calculate the absolute path to the embedding directory
        embedding_path=$(echo "$(pwd)/../../$(dirname {input})")
        
        python {params.pipeline_dir}/cluster.py \\
            --file_pattern "$embedding_path"'/*/*.gz' \\
            --output_file clustering_results.tsv \\
            --min_cluster_size {params.min_cluster_size} \\
            --min_samples {params.min_samples} \\
            --num_cpus {params.threads} \\
            --max_GMM {params.max_GMM} \\
            --seed {params.seed}
            
        touch $(basename {output})
        echo "Completed {wildcards.embedding} clustering"
        """

# Bootstrapping rule - Generates bootstrap samples for each method
rule bootstrapping:
    input:
        "{embedding}/clustering/clustering_complete.txt"
    output:
        "{embedding}/bootstrapping/bootstrapping_complete.txt"
    wildcard_constraints:
        embedding = "RTD|CGR"
    params:
        pipeline_dir = config["pipeline_dir"],
        n_bootstraps = config["n_bootstraps"],
        counts_file = "table.otu",
        volumes_file = "est_volumes.tsv"
    shell:
        """
        echo ""
        echo "============================================================"
        echo "Starting {wildcards.embedding} bootstrapping..."
        echo "============================================================"
        echo "Input directory: $(dirname {input})"
        echo "Output directory: $(dirname {output})"
        echo "Number of bootstraps: {params.n_bootstraps}"
        
        # Get absolute paths before changing directories
        base_dir=$(pwd)
        input_path="$base_dir/$(dirname {input})"
        
        # Create output directory and change to it
        mkdir -p $(dirname {output})
        cd $(dirname {output})
        
        # Run bootstrap with the correct paths
        python {params.pipeline_dir}/bootstrap.py \\
            --counts "$input_path/{params.counts_file}" \\
            --volumes "$input_path/{params.volumes_file}" \\
            --n_bootstraps {params.n_bootstraps}
            
        touch $(basename {output})
        echo "Completed {wildcards.embedding} bootstrapping"
        """

# Sleuth rule - Performs differential expression for each method
rule sleuth_analysis:
    input:
        "{embedding}/bootstrapping/bootstrapping_complete.txt"
    output:
        "{embedding}/Sleuth/sleuth_complete.txt"
    wildcard_constraints:
        embedding = "RTD|CGR"
    params:
        pipeline_dir = config["pipeline_dir"],
        metadata = config["metadata"],
        parameter = config["parameter"],
        pseudocount = config["pseudocount"],
        alpha = config["alpha"],
        foldchange = config["foldchange"]
    shell:
        """
        echo ""
        echo "============================================================"
        echo "Starting {wildcards.embedding} Sleuth analysis..."
        echo "============================================================"
        echo "Bootstrap directory: $(dirname {input})"
        echo "Output directory: $(dirname {output})"
        echo "Statistical parameters:"
        echo "  - Test parameter: {params.parameter}"
        echo "  - Pseudocount: {params.pseudocount}"
        echo "  - Alpha (FDR): {params.alpha}"
        echo "  - Fold change threshold: {params.foldchange}"
        
        # Create output directory
        mkdir -p $(dirname {output})
        
        # First, get the absolute paths before changing directories
        base_dir=$(pwd)
        bootstrap_path="$base_dir/{wildcards.embedding}/bootstrapping"
        
        # Now change to the Sleuth directory to run the analysis
        cd $(dirname {output})
        
        # The Rscript needs:
        # --root: The base directory containing all embedding directories
        # --bootstrap: Just the name of the bootstrap directory, not the full path
        Rscript {params.pipeline_dir}/sleuth.R \
            --root "$base_dir" \
            --metadata {params.metadata} \
            --bootstrap "{wildcards.embedding}/bootstrapping" \
            --pseudocount {params.pseudocount} \
            --alpha {params.alpha} \
            --fc {params.foldchange} \
            --parameter {params.parameter}
            
        # We're already in the output directory, so just touch the completion file
        touch $(basename {output})
        echo "Completed {wildcards.embedding} Sleuth analysis"
        """

# Read retrieval rule - Fetches significant reads for each method
rule read_retrieval:
    input:
        "{embedding}/Sleuth/sleuth_complete.txt"
    output:
        "{embedding}/significant/retrieval_complete.txt"
    wildcard_constraints:
        embedding = "RTD|CGR"
    params:
        pipeline_dir = config["pipeline_dir"],
        threads = config["threads"],
        alpha = config["alpha"],
        foldchange = config["foldchange"],
        paired = "TRUE" if config["paired"] else "FALSE",
        input_dir = INPUT_DIR,
        clusters_file = "clustering_results.tsv",
        cluster_dir = lambda wildcards: f"{wildcards.embedding}/clustering"
    shell:
        """
        echo ""
        echo "============================================================"
        echo "Starting {wildcards.embedding} read retrieval..."
        echo "============================================================"
        echo "Sleuth directory: $(dirname {input})"
        echo "Output directory: $(dirname {output})"
        echo "Significance thresholds:"
        echo "  - Alpha (FDR): {params.alpha}"
        echo "  - Fold change: {params.foldchange}"
        
        # Create output directory
        mkdir -p $(dirname {output})
        
        # Get absolute paths before changing directories
        base_dir=$(pwd)
        sleuth_path="$base_dir/$(dirname {input})"
        cluster_path="$base_dir/{params.cluster_dir}"
        input_path="$base_dir/{params.input_dir}"
        
        # Change to the output directory
        cd $(dirname {output})
        
        # Run the extraction and fetch scripts
        echo "Extracting significant clusters..."
        python {params.pipeline_dir}/extract.py \\
            --clusters "$cluster_path/{params.clusters_file}" \\
            --sleuth "$sleuth_path/plotting.tsv" \\
            --alpha {params.alpha} \\
            --foldchange {params.foldchange}
            
        echo "Fetching reads for significant clusters..."
        {params.pipeline_dir}/fetch_loop.sh \\
            --pipeline {params.pipeline_dir} \\
            --input "$input_path" \\
            --threads {params.threads} \\
            --paired {params.paired}
            
        # We're already in the output directory
        touch $(basename {output})
        echo "Completed {wildcards.embedding} read retrieval"
        
        # If running both methods, provide a helpful message
        if [ "{params.alpha}" ]; then
            echo ""
            echo "Results for {wildcards.embedding} are now available in:"
            echo "  $(pwd)"
        fi
        """

# Optional: Add a summary rule that runs when ALL mode completes both analyses
rule summary_report:
    input:
        # This rule only runs if we're in ALL mode and both methods complete
        expand("{embedding}/significant/retrieval_complete.txt", 
               embedding=EMBEDDINGS_TO_USE) if len(EMBEDDINGS_TO_USE) > 1 else []
    output:
        "comparison_summary.txt" if len(EMBEDDINGS_TO_USE) > 1 else []
    run:
        if len(EMBEDDINGS_TO_USE) > 1:
            print("\n" + "="*70)
            print("DUAL EMBEDDING ANALYSIS COMPLETE")
            print("="*70)
            print("\nBoth RTD and CGR analyses have completed successfully!")
            print("\nYou can now compare results between the two methods:")
            print("  - RTD results: RTD/significant/")
            print("  - CGR results: CGR/significant/")
            print("\nDifferential expression results for each method:")
            print("  - RTD: RTD/Sleuth/plotting.tsv")
            print("  - CGR: CGR/Sleuth/plotting.tsv")
            print("\nConsider comparing:")
            print("  1. Number of significant clusters identified")
            print("  2. Overlap between significant clusters")
            print("  3. Consistency of fold changes")
            print("  4. Method-specific artifacts or biases")
            print("="*70 + "\n")
            
            with open("comparison_summary.txt", "w") as f:
                f.write("Dual embedding analysis completed\n")
                f.write(f"Methods run: {', '.join(EMBEDDINGS_TO_USE)}\n")
                f.write(f"Completion time: {pd.Timestamp.now()}\n")
